'''
2020-12-14 ,ğŸŒ¤ï¼ŒåŒ…å¤´
'''
import requests
from pyquery import PyQuery
import logging
import json

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

start_url = 'http://www.zongheng.com/rank/details.html?rt=1&d=1&i=2&p={page}'#ç½‘é¡µåœ°å€

#è¯·æ±‚å¤´
headers = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36',
}

def get_page(url, proxies):
    '''
    è·å–é¡µé¢ä»£ç 
    :param url: é¡µé¢è¿æ¥
    :param proxies: ä»£ç†ip,å­—å…¸æ ¼å¼
    :return: é¡µé¢çš„ä»£ç å†…å®¹
    '''
    #å‘èµ·è¯·æ±‚
    response = requests.get(url=url, headers=headers, proxies=proxies,  verify=False)
    #å“åº”çŠ¶æ€åˆ¤æ–­
    if response.status_code == 200:
        logging.info("è·å–é¡µé¢ï¼š%s"%url)
        return response.text #è¿”å›é¡µé¢æºç 
    else:
        logging.info("é¡µé¢è·å–å¤±è´¥ï¼š%s"%url)
        return None


def get_detail(text):
    if text is None:
        return None
    #è·å–pyqueryå¯¹è±¡ï¼Œå°†ä¸‹è½½çš„ç½‘é¡µä»£ç ä¼ å…¥
    doc = PyQuery(text)
    #è·å–ä¿¡æ¯å—
    book_lists = doc('.rankpage_box div')
    for book_info in book_lists.items():
        book_name = book_info('.rank_d_list.borderB_c_dsh.clearfix .rank_d_book_intro.fl .rank_d_b_name').attr('title')
        book_url = book_info('.rank_d_list.borderB_c_dsh.clearfix .rank_d_book_intro.fl .rank_d_b_name a').attr('href')
        author_name =  book_info('.rank_d_list.borderB_c_dsh.clearfix .rank_d_book_intro.fl .rank_d_b_cate').attr('title')
        if book_name is None:
            continue
        return {
            'ä¹¦å':book_name,
            'ä½œè€…':author_name,
            'é“¾æ¥':book_url
        }


if __name__ == '__main__':

    proxies = dict() #å­˜æ”¾è¯»å–çš„ä»£ç†ip
    books_info = {"info":list()}#å°†æ¥çš„ä¿¡æ¯å­˜å‚¨åœ¨å­—å…¸ä¸­
    for i in json.load(open('./m_proxies.json'))['proxies']:
        proxies.update({"https":i})

    for i in range(1,11):#çˆ¬å–10é¡µçš„å†…å®¹
        text = get_page(start_url.format(page=i), proxies)
        if text is None:
            continue
        books_info["info"].append(get_detail(text))

    #å°†è·å–çš„å†…å®¹å†™å…¥jsonæ–‡ä»¶
    json.dump(books_info, open('books_info.json', 'a+', encoding='utf-8'), ensure_ascii=False, indent=4)


